{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e4ecad-44d1-42e3-9e88-b6d24fc410b5",
   "metadata": {},
   "source": [
    "# Surrogate model fitting with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c35c0-069b-45d6-84d8-91107808789c",
   "metadata": {},
   "source": [
    "Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30943734-6ec5-4272-b53f-8638b499f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_absolute_percentage_error\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import beta\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Activation, Dropout, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ELU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb5d41-75cf-4a2b-a4e1-be20ed7ba2d7",
   "metadata": {},
   "source": [
    "Load the samples generated by the InteractiveSurrogate notebook.\n",
    "\n",
    "Split into an input table and an output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a478a-07c0-4399-9ad5-8f62e1485362",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pickle.load(open('samples.p', 'rb'))\n",
    "samples_out = samples['Electricity:Facility']\n",
    "samples_in = samples.drop('Electricity:Facility', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dd81a-ef07-48b0-8d66-bd71d58e7bb5",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Split data into training and testing with the ratio 80% training, 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484eea9-952e-4e21-a059-5e797886f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in, test_in, train_out, test_out = train_test_split(samples_in, samples_out, test_size=0.2)\n",
    "# train_out.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c5e13-0613-456f-ab20-8a995f76d6b0",
   "metadata": {},
   "source": [
    "Set up scaling objects that are fitted to the training data and applied to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08615b9-1ba2-4390-b647-ab0a817a379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_zero(array):\n",
    "    array = array + array.max()*0.00001\n",
    "    if array.ndim == 1:\n",
    "        array = array.to_numpy().reshape(-1, 1)\n",
    "    return array\n",
    "\n",
    "\n",
    "scaler_in = MinMaxScaler()\n",
    "NN_inputs = scaler_in.fit_transform(X=avoid_zero(train_in))\n",
    "NN_inputs_test = scaler_in.transform(X=avoid_zero(test_in))\n",
    "\n",
    "scaler_out = StandardScaler() \n",
    "NN_outputs = scaler_out.fit_transform(X=avoid_zero(train_out))\n",
    "NN_outputs_test = scaler_out.transform(X=avoid_zero(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d013e8a-47c4-4790-a385-71d1efbe1f8b",
   "metadata": {},
   "source": [
    "Plot the initial and scaled input and output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a24bf-3e4b-482f-acce-739abbc84fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_in.iloc[:, 0], train_out, 'r.')\n",
    "plt.plot(train_in.iloc[:, 1], train_out, 'b.')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(NN_inputs, NN_outputs, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdaed07-6c9d-4050-b4f0-411b17826315",
   "metadata": {},
   "source": [
    "Construct the model structure using Keras.\n",
    "\n",
    "We add the input layer, a dense layer with 6 nodes and the output layer.\n",
    "\n",
    "Other options currently commented would add inter-layer normalisation, ReLU units to nodes and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520e844-b117-45d8-b106-a6205f24ee11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_inputs = len(NN_inputs[0])\n",
    "print(f'Number of inputs N: {num_inputs}')\n",
    "num_outputs = len(NN_outputs[0])\n",
    "print(f'Number of outputs: {num_outputs}')\n",
    "\n",
    "model = Sequential(name='model')\n",
    "\n",
    "model.add(keras.Input(shape=(num_inputs,)))\n",
    "model.add(Dense(num_inputs*3))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(ReLU())\n",
    "# model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(num_outputs))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ebaa0-64c2-45b9-a867-cc20c43b760a",
   "metadata": {},
   "source": [
    "Set the model fitting parameters.\n",
    "\n",
    "Other possible loss functions include 'mean_absolute_percentage_error' and 'mean_squared_error', see https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567485d-9038-489e-b68d-ef9a718d58bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = 'mean_absolute_error'\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "metrics = ['mae', 'mse', 'mape']\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f44970-06d6-4482-a043-dbc7e2d7b842",
   "metadata": {},
   "source": [
    "Now we can fit the model to the inputs and outputs and visualise the fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161371b3-5fd3-4746-b419-7273d284b452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history=model.fit( x=NN_inputs, y=NN_outputs, batch_size = 25,epochs=500, verbose=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90ef68-b94f-4986-b2a0-a05c63af4848",
   "metadata": {},
   "source": [
    "Define a function to evaluate a dataframe of input values, with pre and ppost scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18508baa-115c-468a-9445-baad37158c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_surrogate(inputs):  # inputs in the form of a pandas dataframe, with columns representing parameter inputs. The inputs are in the original scale and order of the parameter definition.\n",
    "    if inputs.ndim == 1:\n",
    "        inputs = inputs.to_numpy().reshape(-1, 1)\n",
    "    evaluation = scaler_in.transform(inputs)  # transform from original scale to [0,1]\n",
    "    evaluation = model(evaluation, training=False)  # Run inputs through the surrogate\n",
    "    evaluation = scaler_out.inverse_transform(evaluation)  # transform from [0,1] outputs to original scale\n",
    "    if isinstance(samples_out, pd.DataFrame):\n",
    "        evaluation = pd.DataFrame(evaluation, columns=samples_out.columns)  # format into a nice dataframe\n",
    "    else:\n",
    "        evaluation = pd.DataFrame(evaluation, columns=[samples_out.name])\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e367c9-a1cc-4b26-a8c0-23117d88eaaf",
   "metadata": {},
   "source": [
    "Evaluate the test set using the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306f546-faed-4907-8c71-d411a4f1d8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_out = evaluate_surrogate(test_in)\n",
    "model_out.set_index(test_out.index, inplace=True)  # Match the output to the input via indexing\n",
    "test_out = test_out.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef70e1e-cf1b-4426-9f6a-3d6d202bbf66",
   "metadata": {},
   "source": [
    "A scatter plot of true EnergyPlus output values against surrogate model values.\n",
    "\n",
    "The solid black line is the desired 1:1 mapping; the dashed lines show 10% error bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3699984-3825-42f1-89ed-408aa7935cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = 0.1\n",
    "plt.plot(test_out.iloc[:, 0], model_out.iloc[:, 0], '.')\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "plt.plot(np.linspace(xmin, xmax), np.linspace(xmin, xmax), 'black')\n",
    "plt.xlabel(\"EnergyPlus\")\n",
    "plt.ylabel(\"Surrogate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61147860-78e5-4845-94b8-cf66e2e7f226",
   "metadata": {},
   "source": [
    "Calculate the R$^2$ score for the overall model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49fa29-5996-4f05-b8f4-79e7ecce8643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('R2 Score on test set: ' + str(r2_score(test_out.values.flatten(), evaluate_surrogate(test_in).values.flatten(), multioutput='raw_values')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28174472-326e-42a7-aae1-e037af55df35",
   "metadata": {},
   "source": [
    "Calculate the R$^2$ score on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b0441-36dd-42ea-b502-f9343709593d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('R2 Score on training set: ' + str(r2_score(train_out.values.flatten(), evaluate_surrogate(train_in).values.flatten(), multioutput='raw_values')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f73d9c-1fa4-43ae-bec6-a5f01686bcfd",
   "metadata": {},
   "source": [
    "Calculate the maximum test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b35272-b4c8-4957-99f1-8fb40f3b511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors = model_out-test_out  # surrogate - EnergyPlus, so overprediciton is positive\n",
    "test_errors_percent = 100*test_errors/(test_out+0.0001)\n",
    "test_errors_squared = test_errors**2\n",
    "test_errors.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
